# ============================================
# PROGRAM 1 : TIC TAC TOE USING DFS (AI)
# ============================================

# Check winner
def check_winner(board):
    wins = [
        [0,1,2],[3,4,5],[6,7,8],  # rows
        [0,3,6],[1,4,7],[2,5,8],  # columns
        [0,4,8],[2,4,6]           # diagonals
    ]
    for w in wins:
        if board[w[0]] == board[w[1]] == board[w[2]] and board[w[0]] != ' ':
            return board[w[0]]
    return None

# DFS function
def dfs(board, player):
    winner = check_winner(board)

    if winner == 'X':
        return 1
    if winner == 'O':
        return -1
    if ' ' not in board:
        return 0  # draw

    if player == 'X':
        best = -100
        for i in range(9):
            if board[i] == ' ':
                board[i] = 'X'
                score = dfs(board, 'O')
                board[i] = ' '
                best = max(best, score)
        return best
    else:
        best = 100
        for i in range(9):
            if board[i] == ' ':
                board[i] = 'O'
                score = dfs(board, 'X')
                board[i] = ' '
                best = min(best, score)
        return best

# Find best move for AI
def best_move(board):
    best_score = -100
    move = -1
    for i in range(9):
        if board[i] == ' ':
            board[i] = 'X'
            score = dfs(board, 'O')
            board[i] = ' '
            if score > best_score:
                best_score = score
                move = i
    return move

# Display board
def print_board(board):
    for i in range(0,9,3):
        print(board[i], "|", board[i+1], "|", board[i+2])
    print()

# Main game
board = [' '] * 9

while True:
    print_board(board)

    # Human move
    pos = int(input("Enter position (0-8): "))
    board[pos] = 'O'

    if check_winner(board) or ' ' not in board:
        break

    # AI move
    ai_move = best_move(board)
    board[ai_move] = 'X'

    if check_winner(board) or ' ' not in board:
        break

print_board(board)
winner = check_winner(board)
print("Winner:", winner if winner else "Draw")

# ============================================
# PROGRAM 2 : ALPHA-BETA PRUNING (MINIMAX)
# ============================================

import math

def alpha_beta(depth, node_index, is_max, values, alpha, beta):
    
    # Terminal condition (leaf node)
    if depth == math.log2(len(values)):
        return values[node_index]

    if is_max:
        best = -math.inf

        for i in range(2):
            val = alpha_beta(depth + 1, node_index * 2 + i,
                             False, values, alpha, beta)
            best = max(best, val)
            alpha = max(alpha, best)

            # Pruning condition
            if beta <= alpha:
                break

        return best

    else:
        best = math.inf

        for i in range(2):
            val = alpha_beta(depth + 1, node_index * 2 + i,
                             True, values, alpha, beta)
            best = min(best, val)
            beta = min(beta, best)

            # Pruning condition
            if beta <= alpha:
                break

        return best


# Leaf node values
values = [3, 5, 6, 9]

# Call alpha-beta pruning
result = alpha_beta(0, 0, True, values, -math.inf, math.inf)

print("Optimal Value:", result)
# ============================================
# PROGRAM 3 : 8-PUZZLE PROBLEM USING A* SEARCH
# HEURISTIC : MISPLACED TILES
# ============================================


import heapq

# Goal state
goal = (1, 2, 3,
        4, 5, 6,
        7, 8, 0)

# Manhattan Distance heuristic
def heuristic(state):
    distance = 0
    for i in range(9):
        if state[i] != 0:
            goal_pos = goal.index(state[i])
            distance += abs(i//3 - goal_pos//3) + abs(i%3 - goal_pos%3)
    return distance

# Get possible moves
def get_neighbors(state):
    neighbors = []
    zero_pos = state.index(0)
    row, col = zero_pos // 3, zero_pos % 3

    moves = [(-1,0),(1,0),(0,-1),(0,1)]

    for dr, dc in moves:
        new_row, new_col = row + dr, col + dc
        if 0 <= new_row < 3 and 0 <= new_col < 3:
            new_pos = new_row * 3 + new_col
            new_state = list(state)
            new_state[zero_pos], new_state[new_pos] = new_state[new_pos], new_state[zero_pos]
            neighbors.append(tuple(new_state))

    return neighbors

# A* Algorithm
def a_star(start):
    pq = []
    heapq.heappush(pq, (heuristic(start), 0, start))

    visited = set()

    while pq:
        f, g, state = heapq.heappop(pq)

        if state == goal:
            print("Goal reached with cost:", g)
            return

        visited.add(state)

        for neighbor in get_neighbors(state):
            if neighbor not in visited:
                new_g = g + 1
                new_f = new_g + heuristic(neighbor)
                heapq.heappush(pq, (new_f, new_g, neighbor))

    print("No solution found")

# Initial state
start = (1, 2, 3,
         4, 0, 6,
         7, 5, 8)

a_star(start)

# ============================================
# PROGRAM 4 : HILL CLIMBING ALGORITHM
# ============================================

# Hill Climbing to maximize a single variable function

# Function to maximize
def f(x):
    return -x*x + 10*x

# Hill Climbing Algorithm
def hill_climbing(start_x, step_size):
    current_x = start_x
    current_value = f(current_x)

    while True:
        # Check neighbors
        left_x = current_x - step_size
        right_x = current_x + step_size

        left_value = f(left_x)
        right_value = f(right_x)

        # Move towards higher value
        if left_value > current_value:
            current_x = left_x
            current_value = left_value
        elif right_value > current_value:
            current_x = right_x
            current_value = right_value
        else:
            break  # Peak reached

    return current_x, current_value

# Start hill climbing
x, max_value = hill_climbing(start_x=0, step_size=1)

print("Maximum at x =", x)
print("Maximum value f(x) =", max_value)

# ============================================
# PROGRAM 5(A) : LOGISTIC REGRESSION
# DATASET : IRIS (SETOSA vs VERSICOLOR)
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.01, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

iris = load_iris()
X = iris.data
y = iris.target

mask = (y == 0) | (y == 1)
X = X[mask]
y = y[mask]

scaler = StandardScaler()
X = scaler.fit_transform(X)

weights = logistic_regression(X, y)

sl = float(input("Sepal Length: "))
sw = float(input("Sepal Width : "))
pl = float(input("Petal Length: "))
pw = float(input("Petal Width : "))

test = scaler.transform([[sl, sw, pl, pw]])
result = predict(test, weights)

print("Prediction:", "Versicolor" if result[0] else "Setosa")

# ============================================
# PROGRAM 5(B) : LOGISTIC REGRESSION
# DATASET : BREAST CANCER
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.01, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

data = load_breast_cancer()
X = data.data
y = data.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

weights = logistic_regression(X, y)

print("Enter first 5 feature values:")
inputs = [float(input()) for _ in range(5)]

test = np.zeros((1, X.shape[1]))
test[0, :5] = inputs
test = scaler.transform(test)

result = predict(test, weights)
print("Prediction:", "Benign" if result[0] else "Malignant")

# ============================================
# PROGRAM 5(C) : LOGISTIC REGRESSION
# DATASET : CUSTOM
# ============================================

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.1, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

X = np.array([
    [1, 2],
    [2, 3],
    [3, 4],
    [6, 7],
    [7, 8],
    [8, 9]
])

y = np.array([0, 0, 0, 1, 1, 1])

weights = logistic_regression(X, y)

x1 = float(input("Enter x1: "))
x2 = float(input("Enter x2: "))

test = np.array([[x1, x2]])
result = predict(test, weights)

print("Prediction:", result[0])

# ============================================
# PROGRAM 6(A) : NAIVE BAYES
# DATASET : IRIS
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1
)

model = NaiveBayes()
model.fit(X_train, y_train)

predictions = model.predict(X_test)
print("Accuracy:", np.mean(predictions == y_test))
print("Predicted Classes:", iris.target_names[predictions])

# ============================================
# PROGRAM 6(B) : NAIVE BAYES
# DATASET : BREAST CANCER
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1
)

model = NaiveBayes()
model.fit(X_train, y_train)

predictions = model.predict(X_test)
print("Accuracy:", np.mean(predictions == y_test))
print("Predicted Classes:", data.target_names[predictions])

# ============================================
# PROGRAM 6(C) : NAIVE BAYES
# DATASET : CUSTOM
# ============================================

import numpy as np

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

X = np.array([
    [1, 2],
    [1, 3],
    [2, 3],
    [6, 7],
    [7, 8],
    [8, 9]
])

y = np.array([0, 0, 0, 1, 1, 1])

model = NaiveBayes()
model.fit(X, y)

test = np.array([[float(input("Enter x1: ")), float(input("Enter x2: "))]])
prediction = model.predict(test)

print("Prediction:", prediction[0])

# ============================================
# PROGRAM 8(A) : K-MEANS CLUSTERING
# DATASET : IRIS
# ============================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 3
centroids, labels = kmeans(X, k)

colors = ['r', 'g', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.title("K-Means on Iris Dataset")
plt.show()

# ============================================
# PROGRAM 8(B) : K-MEANS CLUSTERING
# DATASET : BREAST CANCER
# ============================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()
X = data.data[:, :2]  # Use first 2 features for plotting

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 2
centroids, labels = kmeans(X, k)

colors = ['r', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("K-Means on Breast Cancer Dataset")
plt.show()

# ============================================
# PROGRAM 8(C) : K-MEANS CLUSTERING
# DATASET : CUSTOM
# ============================================

import numpy as np
import matplotlib.pyplot as plt

X = np.array([
    [1, 2],
    [1, 3],
    [2, 2],
    [6, 7],
    [7, 8],
    [8, 7]
])

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 2
centroids, labels = kmeans(X, k)

colors = ['r', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("K-Means on Custom Dataset")
plt.show()


# ============================================
# PROGRAM 7 : K-NEAREST NEIGHBOR (KNN)
# DATASET : IRIS DATASET
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

data = load_iris()
X = data.data
y = data.target
names = data.target_names

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input
sl = float(input("Sepal length: "))
sw = float(input("Sepal width : "))
pl = float(input("Petal length: "))
pw = float(input("Petal width : "))

test_sample = np.array([sl, sw, pl, pw])

predicted_label = knn(X_train, y_train, test_sample, k=3)
print("Predicted class:", names[predicted_label])

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)


# ============================================
# PROGRAM 7 : K-NEAREST NEIGHBOR (KNN)
# DATASET : TITANIC DATASET
# ============================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Load Titanic dataset (CSV must be in same folder)
data = pd.read_csv("titanic.csv")

# Select required columns and drop missing values
data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']]
data.dropna(inplace=True)

# Convert categorical data to numerical
data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})

# Features and labels
X = data[['Pclass', 'Sex', 'Age', 'Fare']].values
y = data['Survived'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

# Euclidean distance
def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# KNN function
def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input
pclass = int(input("Passenger Class (1/2/3): "))
sex = int(input("Sex (male=0, female=1): "))
age = float(input("Age: "))
fare = float(input("Fare: "))

test_sample = np.array([pclass, sex, age, fare])

predicted = knn(X_train, y_train, test_sample, k=3)
print("Predicted Survival:", predicted)

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)

# ============================================
# PROGRAM 7 : BREAST CANCER PREDICTION USING KNN
# DATASET : BREAST CANCER DATASET (SKLEARN)
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target
names = data.target_names   # ['malignant', 'benign']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

# Euclidean distance
def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# KNN algorithm
def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input (first 5 features only for simplicity)
print("Enter values for breast cancer prediction:")
print("(mean radius, mean texture, mean perimeter, mean area, mean smoothness)")

r = float(input("Mean radius      : "))
t = float(input("Mean texture     : "))
p = float(input("Mean perimeter   : "))
a = float(input("Mean area        : "))
s = float(input("Mean smoothness  : "))

# Pad remaining features with zeros
test_sample = np.zeros(X.shape[1])
test_sample[0:5] = [r, t, p, a, s]

# Prediction
predicted_label = knn(X_train, y_train, test_sample, k=3)
print("Predicted class:", names[predicted_label])

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)
