# ============================================
# PROGRAM 1 : TIC TAC TOE USING DFS (AI)
# ============================================

def initialize_board():
    return [[' ' for _ in range(3)] for _ in range(3)]

def print_board(board):
    print("-------------")
    for row in board:
        print("|", " | ".join(row), "|")
        print("-------------")

def is_winner(board, player):
    for i in range(3):
        if all(board[i][j] == player for j in range(3)):
            return True
        if all(board[j][i] == player for j in range(3)):
            return True

    if all(board[i][i] == player for i in range(3)):
        return True
    if all(board[i][2 - i] == player for i in range(3)):
        return True

    return False

def is_draw(board):
    return all(board[i][j] != ' ' for i in range(3) for j in range(3))

def dfs(board, is_ai_turn):
    if is_winner(board, 'X'):
        return 1
    if is_winner(board, 'O'):
        return -1
    if is_draw(board):
        return 0

    if is_ai_turn:
        best_score = float("-inf")
    else:
        best_score = float("inf")

    for i in range(3):
        for j in range(3):
            if board[i][j] == ' ':
                board[i][j] = 'X' if is_ai_turn else 'O'
                score = dfs(board, not is_ai_turn)
                board[i][j] = ' '

                if is_ai_turn:
                    best_score = max(best_score, score)
                else:
                    best_score = min(best_score, score)

    return best_score

def find_best_move(board):
    best_score = float("-inf")
    best_move = None

    for i in range(3):
        for j in range(3):
            if board[i][j] == ' ':
                board[i][j] = 'X'
                score = dfs(board, False)
                board[i][j] = ' '

                if score > best_score:
                    best_score = score
                    best_move = (i, j)

    return best_move

def get_user_input(board):
    while True:
        try:
            row, col = map(int, input("Enter row and column (0-2): ").split())
            if 0 <= row <= 2 and 0 <= col <= 2 and board[row][col] == ' ':
                return row, col
            else:
                print("Invalid move. Try again.")
        except ValueError:
            print("Enter valid numbers.")

def main():
    board = initialize_board()
    print_board(board)

    while True:
        row, col = get_user_input(board)
        board[row][col] = 'O'
        print_board(board)

        if is_winner(board, 'O'):
            print("You win!")
            break
        if is_draw(board):
            print("Draw!")
            break

        print("AI is making a move...")
        ai_move = find_best_move(board)
        board[ai_move[0]][ai_move[1]] = 'X'
        print_board(board)

        if is_winner(board, 'X'):
            print("AI wins!")
            break
        if is_draw(board):
            print("Draw!")
            break

if __name__ == "__main__":
    main()


# ============================================
# PROGRAM 2 : ALPHA-BETA PRUNING (MINIMAX)
# ============================================

def minimax(node, depth, alpha, beta, maximizingPlayer):
    if depth == 0 or isinstance(node, int):
        return node

    if maximizingPlayer:
        maxEval = float('-inf')
        for child in node:
            if child is None:
                continue
            eval = minimax(child, depth - 1, alpha, beta, False)
            maxEval = max(maxEval, eval)
            alpha = max(alpha, maxEval)
            if beta <= alpha:
                break
        return maxEval
    else:
        minEval = float('inf')
        for child in node:
            if child is None:
                continue
            eval = minimax(child, depth - 1, alpha, beta, True)
            minEval = min(minEval, eval)
            beta = min(beta, minEval)
            if beta <= alpha:
                break
        return minEval

def build_tree(flat_tree, depth):
    if depth == 0 or not flat_tree:
        return flat_tree.pop(0) if flat_tree else None

    children = []
    for _ in range(2):
        if flat_tree:
            children.append(build_tree(flat_tree, depth - 1))
        else:
            children.append(None)
    return children

flattened_tree = list(map(int, input("Enter the flattened game tree: ").split()))
depth = int(input("Enter depth: "))

game_tree = build_tree(flattened_tree, depth)

result = minimax(game_tree, depth, float('-inf'), float('inf'), True)
print("Evaluated Value:", result)

# ============================================
# PROGRAM 3 : 8-PUZZLE PROBLEM USING A* SEARCH
# HEURISTIC : MISPLACED TILES
# ============================================

import heapq

def misplaced_tiles(state, goal):
    count = 0
    for i in range(3):
        for j in range(3):
            if state[i][j] != 0 and state[i][j] != goal[i][j]:
                count += 1
    return count

def get_neighbors(state):
    neighbors = []
    x, y = next((i, j) for i in range(3) for j in range(3) if state[i][j] == 0)

    moves = [(x - 1, y), (x + 1, y), (x, y - 1), (x, y + 1)]

    for nx, ny in moves:
        if 0 <= nx < 3 and 0 <= ny < 3:
            new_state = [list(row) for row in state]
            new_state[x][y], new_state[nx][ny] = new_state[nx][ny], new_state[x][y]
            neighbors.append(tuple(tuple(row) for row in new_state))

    return neighbors

def a_star(start, goal):
    open_set = []
    heapq.heappush(open_set, (0, start, 0, None))

    visited = set()
    parents = {}

    while open_set:
        _, current, g, parent = heapq.heappop(open_set)

        if current in visited:
            continue

        visited.add(current)
        parents[current] = parent

        if current == goal:
            path = []
            while current:
                path.append(current)
                current = parents[current]
            return path[::-1]

        for neighbor in get_neighbors(current):
            if neighbor not in visited:
                h = misplaced_tiles(neighbor, goal)
                heapq.heappush(open_set, (g + 1 + h, neighbor, g + 1, current))

    return None

def parse_input(input_string):
    try:
        numbers = list(map(int, input_string.split()))
        if sorted(numbers) != list(range(9)):
            raise ValueError("Input must contain numbers 0 to 8 exactly once")
        return tuple(tuple(numbers[i:i + 3]) for i in range(0, 9, 3))
    except Exception as e:
        print("Invalid input:", e)
        return None

if __name__ == "__main__":
    print("Enter initial state (e.g., 1 2 3 4 0 5 6 7 8):")
    initial_input = input().strip()

    print("Enter goal state (e.g., 1 2 3 4 5 6 7 8 0):")
    goal_input = input().strip()

    start_state = parse_input(initial_input)
    goal_state = parse_input(goal_input)

    if start_state and goal_state:
        solution = a_star(start_state, goal_state)
        if solution:
            print("Solution found:")
            for step in solution:
                for row in step:
                    print(row)
                print()
        else:
            print("No solution exists.")
    else:
        print("Invalid input.")


# ============================================
# PROGRAM 4 : HILL CLIMBING ALGORITHM
# ============================================

def hill_climbing(func, start, step_size=0.01, max_iterations=1000):
    current_position = start
    current_value = func(current_position)

    for i in range(max_iterations):
        next_position_positive = current_position + step_size
        next_value_positive = func(next_position_positive)

        next_position_negative = current_position - step_size
        next_value_negative = func(next_position_negative)

        if next_value_positive > current_value and next_value_positive >= next_value_negative:
            current_position = next_position_positive
            current_value = next_value_positive
        elif next_value_negative > current_value and next_value_negative > next_value_positive:
            current_position = next_position_negative
            current_value = next_value_negative
        else:
            break

    return current_position, current_value


# Get the function from the user
while True:
    func_str = input("Enter a function of x: ")
    try:
        x = 0
        eval(func_str)
        break
    except Exception:
        print("Invalid function. Try again.")

func = lambda x: eval(func_str)

# Get starting point
while True:
    start_str = input("Enter starting value: ")
    try:
        start = float(start_str)
        break
    except ValueError:
        print("Invalid input. Enter a number.")

maxima, max_value = hill_climbing(func, start)
print("The maxima is at x =", maxima)
print("The maximum value obtained is", max_value)

# ============================================
# PROGRAM 5(A) : LOGISTIC REGRESSION
# DATASET : IRIS (SETOSA vs VERSICOLOR)
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.01, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

iris = load_iris()
X = iris.data
y = iris.target

mask = (y == 0) | (y == 1)
X = X[mask]
y = y[mask]

scaler = StandardScaler()
X = scaler.fit_transform(X)

weights = logistic_regression(X, y)

sl = float(input("Sepal Length: "))
sw = float(input("Sepal Width : "))
pl = float(input("Petal Length: "))
pw = float(input("Petal Width : "))

test = scaler.transform([[sl, sw, pl, pw]])
result = predict(test, weights)

print("Prediction:", "Versicolor" if result[0] else "Setosa")

# ============================================
# PROGRAM 5(B) : LOGISTIC REGRESSION
# DATASET : BREAST CANCER
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.01, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

data = load_breast_cancer()
X = data.data
y = data.target

scaler = StandardScaler()
X = scaler.fit_transform(X)

weights = logistic_regression(X, y)

print("Enter first 5 feature values:")
inputs = [float(input()) for _ in range(5)]

test = np.zeros((1, X.shape[1]))
test[0, :5] = inputs
test = scaler.transform(test)

result = predict(test, weights)
print("Prediction:", "Benign" if result[0] else "Malignant")

# ============================================
# PROGRAM 5(C) : LOGISTIC REGRESSION
# DATASET : CUSTOM
# ============================================

import numpy as np

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def logistic_regression(X, y, lr=0.1, epochs=1000):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    weights = np.zeros(X.shape[1])

    for _ in range(epochs):
        z = np.dot(X, weights)
        h = sigmoid(z)
        gradient = np.dot(X.T, (h - y)) / y.size
        weights -= lr * gradient

    return weights

def predict(X, weights):
    X = np.hstack((np.ones((X.shape[0], 1)), X))
    return sigmoid(np.dot(X, weights)) >= 0.5

X = np.array([
    [1, 2],
    [2, 3],
    [3, 4],
    [6, 7],
    [7, 8],
    [8, 9]
])

y = np.array([0, 0, 0, 1, 1, 1])

weights = logistic_regression(X, y)

x1 = float(input("Enter x1: "))
x2 = float(input("Enter x2: "))

test = np.array([[x1, x2]])
result = predict(test, weights)

print("Prediction:", result[0])

# ============================================
# PROGRAM 6(A) : NAIVE BAYES
# DATASET : IRIS
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1
)

model = NaiveBayes()
model.fit(X_train, y_train)

predictions = model.predict(X_test)
print("Accuracy:", np.mean(predictions == y_test))
print("Predicted Classes:", iris.target_names[predictions])

# ============================================
# PROGRAM 6(B) : NAIVE BAYES
# DATASET : BREAST CANCER
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

data = load_breast_cancer()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=1
)

model = NaiveBayes()
model.fit(X_train, y_train)

predictions = model.predict(X_test)
print("Accuracy:", np.mean(predictions == y_test))
print("Predicted Classes:", data.target_names[predictions])

# ============================================
# PROGRAM 6(C) : NAIVE BAYES
# DATASET : CUSTOM
# ============================================

import numpy as np

class NaiveBayes:
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.mean = np.array([X[y == c].mean(axis=0) for c in self.classes])
        self.var = np.array([X[y == c].var(axis=0) for c in self.classes])
        self.priors = np.array([X[y == c].shape[0] / len(y) for c in self.classes])

    def predict(self, X):
        return np.array([self._predict(x) for x in X])

    def _predict(self, x):
        posteriors = []
        for i in range(len(self.classes)):
            prior = np.log(self.priors[i])
            likelihood = np.sum(np.log(self._pdf(i, x)))
            posteriors.append(prior + likelihood)
        return self.classes[np.argmax(posteriors)]

    def _pdf(self, class_idx, x):
        mean = self.mean[class_idx]
        var = self.var[class_idx]
        return np.exp(- (x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)

X = np.array([
    [1, 2],
    [1, 3],
    [2, 3],
    [6, 7],
    [7, 8],
    [8, 9]
])

y = np.array([0, 0, 0, 1, 1, 1])

model = NaiveBayes()
model.fit(X, y)

test = np.array([[float(input("Enter x1: ")), float(input("Enter x2: "))]])
prediction = model.predict(test)

print("Prediction:", prediction[0])

# ============================================
# PROGRAM 8(A) : K-MEANS CLUSTERING
# DATASET : IRIS
# ============================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 3
centroids, labels = kmeans(X, k)

colors = ['r', 'g', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("Sepal Length")
plt.ylabel("Sepal Width")
plt.title("K-Means on Iris Dataset")
plt.show()

# ============================================
# PROGRAM 8(B) : K-MEANS CLUSTERING
# DATASET : BREAST CANCER
# ============================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer

data = load_breast_cancer()
X = data.data[:, :2]  # Use first 2 features for plotting

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 2
centroids, labels = kmeans(X, k)

colors = ['r', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title("K-Means on Breast Cancer Dataset")
plt.show()

# ============================================
# PROGRAM 8(C) : K-MEANS CLUSTERING
# DATASET : CUSTOM
# ============================================

import numpy as np
import matplotlib.pyplot as plt

X = np.array([
    [1, 2],
    [1, 3],
    [2, 2],
    [6, 7],
    [7, 8],
    [8, 7]
])

def kmeans(X, k):
    centroids = X[np.random.choice(X.shape[0], k, replace=False)]

    for _ in range(100):
        distances = np.linalg.norm(X[:, None] - centroids, axis=2)
        labels = np.argmin(distances, axis=1)
        centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    return centroids, labels

k = 2
centroids, labels = kmeans(X, k)

colors = ['r', 'b']
for i in range(k):
    plt.scatter(X[labels == i, 0], X[labels == i, 1], c=colors[i])

plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x')
plt.xlabel("X")
plt.ylabel("Y")
plt.title("K-Means on Custom Dataset")
plt.show()


# ============================================
# PROGRAM 7 : K-NEAREST NEIGHBOR (KNN)
# DATASET : IRIS DATASET
# ============================================

import numpy as np
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

data = load_iris()
X = data.data
y = data.target
names = data.target_names

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input
sl = float(input("Sepal length: "))
sw = float(input("Sepal width : "))
pl = float(input("Petal length: "))
pw = float(input("Petal width : "))

test_sample = np.array([sl, sw, pl, pw])

predicted_label = knn(X_train, y_train, test_sample, k=3)
print("Predicted class:", names[predicted_label])

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)


# ============================================
# PROGRAM 7 : K-NEAREST NEIGHBOR (KNN)
# DATASET : TITANIC DATASET
# ============================================

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Load Titanic dataset (CSV must be in same folder)
data = pd.read_csv("titanic.csv")

# Select required columns and drop missing values
data = data[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']]
data.dropna(inplace=True)

# Convert categorical data to numerical
data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})

# Features and labels
X = data[['Pclass', 'Sex', 'Age', 'Fare']].values
y = data['Survived'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

# Euclidean distance
def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# KNN function
def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input
pclass = int(input("Passenger Class (1/2/3): "))
sex = int(input("Sex (male=0, female=1): "))
age = float(input("Age: "))
fare = float(input("Fare: "))

test_sample = np.array([pclass, sex, age, fare])

predicted = knn(X_train, y_train, test_sample, k=3)
print("Predicted Survival:", predicted)

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)

# ============================================
# PROGRAM 7 : BREAST CANCER PREDICTION USING KNN
# DATASET : BREAST CANCER DATASET (SKLEARN)
# ============================================

import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target
names = data.target_names   # ['malignant', 'benign']

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1
)

# Euclidean distance
def euclidean(a, b):
    return np.sqrt(np.sum((a - b) ** 2))

# KNN algorithm
def knn(X_train, y_train, test_point, k):
    distances = []

    for i in range(len(X_train)):
        d = euclidean(test_point, X_train[i])
        distances.append((d, y_train[i]))

    distances.sort(key=lambda x: x[0])

    votes = {}
    for i in range(k):
        label = distances[i][1]
        votes[label] = votes.get(label, 0) + 1

    return max(votes, key=votes.get)

# User input (first 5 features only for simplicity)
print("Enter values for breast cancer prediction:")
print("(mean radius, mean texture, mean perimeter, mean area, mean smoothness)")

r = float(input("Mean radius      : "))
t = float(input("Mean texture     : "))
p = float(input("Mean perimeter   : "))
a = float(input("Mean area        : "))
s = float(input("Mean smoothness  : "))

# Pad remaining features with zeros
test_sample = np.zeros(X.shape[1])
test_sample[0:5] = [r, t, p, a, s]

# Prediction
predicted_label = knn(X_train, y_train, test_sample, k=3)
print("Predicted class:", names[predicted_label])

# Accuracy calculation
correct = 0
for i in range(len(X_test)):
    pred = knn(X_train, y_train, X_test[i], k=3)
    if pred == y_test[i]:
        correct += 1

accuracy = correct / len(X_test)
print("Accuracy:", accuracy)
